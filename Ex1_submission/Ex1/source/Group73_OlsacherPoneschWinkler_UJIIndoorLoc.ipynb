{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f471b95d",
   "metadata": {},
   "source": [
    "# UJIIndoorLoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c41032",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "\n",
    "requirements_path = os.path.join(os.getcwd(), 'requirements.txt')\n",
    "\n",
    "os.system(f'pip install -r {requirements_path}')\n",
    "print(\"All dependencies have been installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import sklearn measures\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e4abe4",
   "metadata": {},
   "source": [
    "# 1. Read Data into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46664121",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafolder = \"../../datasets/UJIIndoorLoc\"\n",
    "\n",
    "trainfile = os.path.join(datafolder, \"trainingData.csv\")\n",
    "validfile = os.path.join(datafolder, \"validationData.csv\")\n",
    "\n",
    "#read data into pandas dataframes\n",
    "train_data = pd.read_csv(trainfile)\n",
    "test_data = pd.read_csv(validfile)\n",
    "print(\"Training data shape:\", train_data.shape)\n",
    "test_data.head()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936e592f",
   "metadata": {},
   "source": [
    "## 1.2 Split Target and Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5e5a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response variables in our problem are Building, Floor, Latitude, Longitude and Relative Position\n",
    "train_data[['FLOOR','BUILDINGID', 'SPACEID','RELATIVEPOSITION','USERID','PHONEID']].astype(str).describe(include=['object'])\n",
    "test_data[['FLOOR','BUILDINGID', 'SPACEID','RELATIVEPOSITION','USERID','PHONEID']].astype(str).describe(include=['object'])\n",
    "\n",
    "## X and Y-data split\n",
    "# Features are the WAP columns\n",
    "X_train = train_data.iloc[:,:520]\n",
    "X_test = test_data.iloc[:,:520]\n",
    "\n",
    "# Select FLOOR and BUILDINGID for location prediction\n",
    "y_train = train_data[['FLOOR', 'BUILDINGID', 'SPACEID']].copy()\n",
    "y_test  = test_data[['FLOOR', 'BUILDINGID']].copy()\n",
    "y_sID = y_train.copy()\n",
    "\n",
    "\n",
    "\n",
    "#histogram of space ids for test set\n",
    "# Combine BUILDINGID and FLOOR into a simpler location code\n",
    "# Format: \"B{BUILDING}F{FLOOR}\" for better readability\n",
    "y_sID['LOC_CODE'] = (y_sID['BUILDINGID'].astype(str) + \n",
    "                       y_sID['FLOOR'].astype(str)+\n",
    "                       y_sID['SPACEID'].astype(str)\n",
    "                       )\n",
    "y_train['LOC_CODE'] = (y_train['BUILDINGID'].astype(str) + \n",
    "                       y_train['FLOOR'].astype(str)\n",
    "                       )\n",
    "y_test['LOC_CODE'] = (y_test['BUILDINGID'].astype(str) + \n",
    "                      y_test['FLOOR'].astype(str) \n",
    "                      )\n",
    "\n",
    "# Keep only the combined column\n",
    "y_train_init = y_train[['LOC_CODE']].astype('category')\n",
    "y_test_init  = y_test[['LOC_CODE']].astype('category')\n",
    "y_sID = y_sID[['LOC_CODE']].astype('category')\n",
    "\n",
    "\n",
    "\n",
    "y_sID.shape,X_train.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428f8612",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "drop NaN and show distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d080b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw_train = X_train\n",
    "X_raw_test = X_test\n",
    "\n",
    "# fill 100 with NaN for further processing\n",
    "# Transform Train data\n",
    "X_raw_train = (X_raw_train\n",
    "\t\t\t .replace(to_replace=100,value=np.nan))\n",
    "X_raw_test = (X_raw_test\n",
    "\t\t\t .replace(to_replace=100,value=np.nan))\n",
    "\n",
    "\n",
    "X_raw_combined = pd.concat([X_raw_train, X_raw_test], axis=0)\n",
    "Y_raw_combined = pd.concat([y_train_init, y_test_init], axis=0)\n",
    "\n",
    "# new train/test split from the combined dataset (used as overall \"tot_split\")\n",
    "X_train_tot_split, X_test_tot_split, Y_train_tot_split, Y_test_tot_split = train_test_split(\n",
    "\tX_raw_combined, Y_raw_combined, test_size=0.2, random_state=42, stratify=Y_raw_combined\n",
    ")\n",
    "\n",
    "# create the train/validation split used elsewhere as \"split_new\"\n",
    "# use the original training set and the location codes including SPACEID (y_train_new_predict)\n",
    "X_train_split_sID, X_test_split_sID, Y_train_split_sID, Y_test_split_sID = train_test_split(\n",
    "\tX_train, y_sID, test_size=0.2, random_state=42, stratify=y_sID\n",
    ")\n",
    "\n",
    "# PCA on the split_new (for SPACEID experiments)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=0.95, random_state=42)\n",
    "X_train_pca = pca.fit_transform(X_train_split_sID)\n",
    "X_test_pca = pca.transform(X_test_split_sID)\n",
    "print(f\"Original number of features: {X_train_split_sID.shape[1]}\")\n",
    "print(f\"Reduced number of features after PCA: {X_train_pca.shape[1]}\")\n",
    "\n",
    "# fill NaNs for algorithms that require no missing values\n",
    "X_train_noNAN = X_train_split_sID.fillna(-110)\n",
    "X_test_noNAN = X_test_split_sID.fillna(-110)\n",
    "\n",
    "# convert PCA results back to DataFrame for consistency\n",
    "X_train_pca = pd.DataFrame(X_train_pca)\n",
    "X_test_pca = pd.DataFrame(X_test_pca)\n",
    "\n",
    "# show shapes to verify consistency\n",
    "X_raw_train.shape, X_raw_test.shape, X_train_split_sID.shape, X_test_split_sID.shape, Y_train_split_sID.shape, Y_test_split_sID.shape, X_train_pca.shape, X_test_pca.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392b97f5",
   "metadata": {},
   "source": [
    "### Initial Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43fc82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "waps_in_range_train = (X_raw_train\n",
    "                 .notnull()\n",
    "                 .sum(axis = 1))\n",
    "\n",
    "waps_in_range_test = (X_raw_test\n",
    "                 .notnull()\n",
    "                 .sum(axis = 1))\n",
    "\n",
    "waps_in_range_tot_train_split = (X_train_tot_split\n",
    "                 .notnull() \n",
    "                 .sum(axis = 1))\n",
    "waps_in_range_tot_test_split = (X_test_tot_split\n",
    "                 .notnull() \n",
    "                 .sum(axis = 1))\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "\n",
    "sns.violinplot(waps_in_range_test, ax = ax, label = 'Test Data')\n",
    "ax.set_xlabel(\"Number of APs in range\")\n",
    "ax.legend()\n",
    "waps_in_range_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8bd2c2",
   "metadata": {},
   "source": [
    "### Initial Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5eca26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "\n",
    "sns.violinplot(waps_in_range_train, ax = ax, label = 'Test Data')\n",
    "ax.set_xlabel(\"Number of APs in range\")\n",
    "ax.legend()\n",
    "waps_in_range_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c116b5b7",
   "metadata": {},
   "source": [
    "### New Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dd7ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "\n",
    "sns.violinplot(waps_in_range_tot_test_split, ax = ax, label = 'Test Data')\n",
    "ax.set_xlabel(\"Number of APs in range\")\n",
    "ax.legend()\n",
    "waps_in_range_tot_test_split.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8735e94e",
   "metadata": {},
   "source": [
    "### New Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6784b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "\n",
    "\n",
    "sns.violinplot(waps_in_range_tot_train_split, ax = ax, label = 'Train Data')\n",
    "ax.set_xlabel(\"Number of APs in range\")\n",
    "ax.legend()\n",
    "waps_in_range_tot_train_split.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c297a8bc",
   "metadata": {},
   "source": [
    "### Initial not NaN-Values distribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04107298",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stack = X_raw_train.stack(future_stack=True)\n",
    "sns.histplot(X_stack.dropna(),kde = False)\n",
    "X_stack.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2883110",
   "metadata": {},
   "source": [
    "## Do Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d42140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "def make_preprocessor(scale: bool = False):\n",
    "    \"\"\"\n",
    "    Return a transformer that imputes missing WAPs to -110 and\n",
    "    optionally applies StandardScaler to all columns.\n",
    "    Use in Pipeline as: ('preprocessor', make_preprocessor(scale=...))\n",
    "    \"\"\"\n",
    "    if scale:\n",
    "        transformer = Pipeline([\n",
    "            (\"imp\", SimpleImputer(strategy=\"constant\", fill_value=-110)),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ])\n",
    "    else:\n",
    "        transformer = SimpleImputer(strategy=\"constant\", fill_value=-110)\n",
    "\n",
    "    # Apply to all columns; ColumnTransformer is a valid transformer for Pipeline\n",
    "    return ColumnTransformer(\n",
    "        [(\"all_waps\", transformer, slice(None))],\n",
    "        remainder=\"drop\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d94bc08",
   "metadata": {},
   "source": [
    "## Grid Search for \"best\" parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ece690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence noisy worker warnings\n",
    "import os, warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".pkg_resources is deprecated as an API.\", category=UserWarning)\n",
    "os.environ.setdefault(\"PYTHONWARNINGS\", \"ignore:pkg_resources is deprecated as an API:UserWarning\")\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "from classifierConfigs import get_classifier_configs, get_scorings, model_requires_int_labels\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "##reload classifierConfigs\n",
    "import importlib\n",
    "import classifierConfigs as cc\n",
    "importlib.reload(cc)\n",
    "\n",
    "get_classifier_configs = cc.get_classifier_configs\n",
    "get_scorings = cc.get_scorings\n",
    "model_requires_int_labels = cc.model_requires_int_labels\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Use the train/validation split already computed earlier in the notebook\n",
    "# (X_train_split_new, X_test_split_new, Y_train_split_new, Y_test_split_new)\n",
    "# Ensure label Series are 1D and contain the LOC_CODE category\n",
    "\n",
    "\n",
    "X_train = X_raw_train\n",
    "X_val = X_raw_test\n",
    "y_train = y_train_init['LOC_CODE'].astype('category')\n",
    "y_val = y_test_init['LOC_CODE'].astype('category')\n",
    "\n",
    "# Integer-coded labels for models that require them (e.g., some XGBoost multiclass setups)\n",
    "y_train_int = y_train.cat.codes\n",
    "y_val_int = y_val.cat.codes\n",
    "\n",
    "# Task selection: 'multiclass' or 'binary'\n",
    "task = 'multiclass'\n",
    "\n",
    "# Multi-metric scoring and configs by task\n",
    "scoring = get_scorings(multiclass=(task == 'multiclass'))\n",
    "refit_metric = \"f1_macro\"\n",
    "configs = get_classifier_configs(task=task)\n",
    "\n",
    "results_dir = \"artifacts/gridsearch\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "all_results = []\n",
    "\n",
    "for name, cfg in configs.items():\n",
    "    print(f\"Running Grid Search for {name}...\")\n",
    "    scale_TF = name.lower().startswith('knn')  # Scale only for KNN\n",
    "    pipe = Pipeline(steps=[\n",
    "        ('preprocessor', make_preprocessor(scale=scale_TF)),\n",
    "        ('classifier', cfg['model'])\n",
    "    ])\n",
    "\n",
    "    param_grid = {f'classifier__{k}': v for k, v in cfg['param_grid'].items()}\n",
    "\n",
    "    gs = GridSearchCV(\n",
    "        estimator=pipe,\n",
    "        param_grid=param_grid,\n",
    "        scoring=scoring,\n",
    "        refit=refit_metric,\n",
    "        n_jobs=1,\n",
    "        verbose=3,\n",
    "        cv=3\n",
    "    )\n",
    "\n",
    "    # Pick correct label vector for fitting: integer codes only if the model requires them\n",
    "    y_fit = y_train_int if (task == 'multiclass' and model_requires_int_labels(name)) else y_train\n",
    "    gs.fit(X_train, y_fit)\n",
    "\n",
    "    # Save per-model CV results\n",
    "    res_df = pd.DataFrame(gs.cv_results_)\n",
    "    rank_col = f'rank_test_{refit_metric}' if f'rank_test_{refit_metric}' in res_df.columns else ('rank_test_score' if 'rank_test_score' in res_df.columns else None)\n",
    "    if rank_col is not None:\n",
    "        res_df = res_df.sort_values(rank_col)\n",
    "    res_df.insert(0, 'model', name)\n",
    "    res_df.insert(1, 'scoring', refit_metric)\n",
    "    res_df['preproc_scaled'] = scale_TF\n",
    "    out_path = os.path.join(results_dir, f\"{name.replace(' ', '')}_cv_results{refit_metric}.csv\")\n",
    "    res_df.to_csv(out_path, index=False)\n",
    "    all_results.append(res_df)\n",
    "\n",
    "    print(f\"Best parameters for {name}: {gs.best_params_} | Best {refit_metric}: {gs.best_score_:.4f}\")\n",
    "\n",
    "    # Predict on validation set and decode integer labels back to category names if needed\n",
    "    y_val_pred = gs.predict(X_val)\n",
    "    if task == 'multiclass' and model_requires_int_labels(name):\n",
    "        # Map integer codes back to category labels using the training categories\n",
    "        categories = y_train.cat.categories\n",
    "        y_val_pred = pd.Series(y_val_pred).map(lambda c: categories[c] if (isinstance(c, (int, np.integer)) and c < len(categories)) else str(c)).values\n",
    "\n",
    "    # Ensure y_val is plain array-like of labels (strings/categories) for reporting\n",
    "    y_val_labels = y_val.values if hasattr(y_val, 'values') else y_val\n",
    "\n",
    "    print(f\"Validation Classification Report for {name}:\\n\")\n",
    "    print(classification_report(y_val_labels, y_val_pred, zero_division=0))\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "if all_results:\n",
    "    pd.concat(all_results, ignore_index=True).to_csv(\n",
    "        os.path.join(results_dir, f\"UJIIndoorLoc_ALL_cv_results_{refit_metric}.csv\"),\n",
    "        index=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247829e5",
   "metadata": {},
   "source": [
    "## KNN-Classifier using Parameters from Grid Search\n",
    "#### single run for preprocessing differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ac1cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "#get preprocessor function\n",
    "print(\"Preparing data with SpaceID \\n\")\n",
    "X_preprocessor = make_preprocessor(scale=True)\n",
    "X_std_train = X_preprocessor.fit_transform(X_train_split_sID)\n",
    "X_std_test = X_preprocessor.transform(X_test_split_sID)\n",
    "\n",
    "\n",
    "# Convert categorical LOC_CODE to numeric codes\n",
    "y_train_codes = Y_train_split_sID['LOC_CODE'].cat.codes\n",
    "y_test_codes = Y_test_split_sID['LOC_CODE'].cat.codes\n",
    "\n",
    "\n",
    "\n",
    "#=============================================================================\n",
    "# Train classifier with SpaceID and scaling\n",
    "#=============================================================================\n",
    "knn_clf = KNeighborsClassifier(n_neighbors=2, n_jobs=-1, p=1, weights='distance')\n",
    "knn_clf.fit(X_std_train, y_train_codes)\n",
    "\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_knn = knn_clf.predict(X_std_test)\n",
    "\n",
    "# Evaluate\n",
    "print(\"=============================================================================\")\n",
    "print(\"With Scaling Results:\")\n",
    "accuracy_knn = accuracy_score(y_test_codes, y_pred_knn)\n",
    "f1_knn = f1_score(y_test_codes, y_pred_knn, average='weighted')\n",
    "print(f\"\\nKNN Accuracy: {accuracy_knn:.4f}\")\n",
    "print(f\"KNN F1 Score: {f1_knn:.4f}\")\n",
    "\n",
    "\n",
    "#=============================================================================\n",
    "# Train classifier without scaling and with SpaceID\n",
    "#=============================================================================\n",
    "X_woscaling = make_preprocessor(scale=False)\n",
    "X_woscl_train = X_woscaling.fit_transform(X_train_split_sID)\n",
    "X_woscl_test = X_woscaling.transform(X_test_split_sID)\n",
    "\n",
    "knn_clf_wo_scaling = KNeighborsClassifier(n_neighbors=2, n_jobs=-1, p=1, weights='distance')\n",
    "knn_clf_wo_scaling.fit(X_woscl_train, y_train_codes)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_knn_wo_scaling = knn_clf_wo_scaling.predict(X_woscl_test)\n",
    "\n",
    "# Evaluate\n",
    "accuracy_knn_wo_scaling = accuracy_score(y_test_codes, y_pred_knn_wo_scaling)\n",
    "f1_knn_wo_scaling = f1_score(y_test_codes, y_pred_knn_wo_scaling, average='weighted')\n",
    "print(\"=============================================================================\")\n",
    "print(\"Without Scaling Results:\")\n",
    "print(f\"\\nKNN without Scaling Accuracy: {accuracy_knn_wo_scaling:.4f}\")\n",
    "print(f\"KNN without Scaling F1 Score: {f1_knn_wo_scaling:.4f}\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# KNN without SpaceID\n",
    "# =============================================================================\n",
    "\n",
    "X_preprocessor_new_split = make_preprocessor(scale=True)\n",
    "X_std_train_wospaceID = X_preprocessor_new_split.fit_transform(X_train_tot_split)\n",
    "X_std_test_wospaceID = X_preprocessor_new_split.transform(X_test_tot_split)\n",
    "\n",
    "y_train_codes_wospaceID = Y_train_tot_split['LOC_CODE'].cat.codes\n",
    "y_test_codes_wospaceID = Y_test_tot_split['LOC_CODE'].cat.codes\n",
    "\n",
    "knn_clf_wospaceID = KNeighborsClassifier(n_neighbors=2, n_jobs=-1, p=1, weights='distance')\n",
    "knn_clf_wospaceID.fit(X_std_train_wospaceID, y_train_codes_wospaceID)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_knn_wospaceID = knn_clf_wospaceID.predict(X_std_test_wospaceID)\n",
    "# Evaluate\n",
    "accuracy_knn_wospaceID = accuracy_score(y_test_codes_wospaceID, y_pred_knn_wospaceID)\n",
    "f1_knn_wospaceID = f1_score(y_test_codes_wospaceID, y_pred_knn_wospaceID, average='weighted')\n",
    "print(\"\\n\")\n",
    "print(\"=============================================================================\")\n",
    "print(\"KNN Results without SpaceID:\")\n",
    "print(\"=============================================================================\")\n",
    "print(\"Without SpaceID Results:\")\n",
    "print(f\"\\nKNN without SpaceID Accuracy: {accuracy_knn_wospaceID:.4f}\")\n",
    "print(f\"KNN without SpaceID F1 Score: {f1_knn_wospaceID:.4f}\\n\")\n",
    "\n",
    "#==================\n",
    "# KNN initial split\n",
    "#==================\n",
    "\n",
    "\n",
    "X_train_init = X_raw_train \n",
    "X_test_init = X_raw_test \n",
    "\n",
    "\n",
    "X_preprocessor_init = make_preprocessor(scale=True)\n",
    "X_std_train_init = X_preprocessor_init.fit_transform(X_train_init)\n",
    "X_std_test_init = X_preprocessor_init.transform(X_test_init)\n",
    "\n",
    "knn_clf_init = KNeighborsClassifier(n_neighbors=2, n_jobs=-1, p=1, weights='distance')\n",
    "knn_clf_init.fit(X_std_train_init, y_train_init)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_knn_init = knn_clf_init.predict(X_std_test_init)\n",
    "# Evaluate\n",
    "accuracy_knn_init = accuracy_score(y_test_init, y_pred_knn_init)\n",
    "f1_knn_init = f1_score(y_test_init, y_pred_knn_init, average='weighted')\n",
    "print(\"\\n\")\n",
    "print(\"=============================================================================\")\n",
    "print(\"With initial Test/Train-Split Results:\")\n",
    "print(f\"\\nKNN without SpaceID Accuracy: {accuracy_knn_init:.4f}\")\n",
    "print(f\"KNN without SpaceID F1 Score: {f1_knn_init:.4f}\\n\")\n",
    "\n",
    "#write results to a csv file\n",
    "results = {\n",
    "    'Model': ['KNN with SpaceID and Scaling', 'KNN with SpaceID without Scaling', 'KNN without SpaceID', 'KNN initial split'],\n",
    "    'Accuracy': [accuracy_knn, accuracy_knn_wo_scaling, accuracy_knn_wospaceID, accuracy_knn_init],\n",
    "    'F1 Score': [f1_knn, f1_knn_wo_scaling, f1_knn_wospaceID, f1_knn_init]\n",
    "}\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv('artifacts/knn_sgl_ujiindoorloc_results.csv', index=False)\n",
    "print(\"Results have been saved to 'artifacts/knn_sgl_ujiindoorloc_results.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d5c2cc",
   "metadata": {},
   "source": [
    "## Random Forest using parameter from Grid Search\n",
    "#### single run for preprocessing differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b98054",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Use the train/validation split already computed earlier in the notebook\n",
    "# (X_train_split_sID, X_test_split_sID, Y_train_split_sID, Y_test_split_sID)\n",
    "\n",
    "X_train = X_train_split_sID\n",
    "X_val = X_test_split_sID\n",
    "y_train = Y_train_split_sID['LOC_CODE'].astype('category')\n",
    "y_val = Y_test_split_sID['LOC_CODE'].astype('category')\n",
    "\n",
    "# Integer-coded labels for models that require them (e.g., some Random Forest multiclass setups)\n",
    "y_train_int = y_train.cat.codes\n",
    "y_val_int = y_val.cat.codes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =================================================================================================================================================\n",
    "# Random Forest with SpaceID\n",
    "# =============================================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training Random Forest with SpaceID...\")\n",
    "clf = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1, max_features='log2')\n",
    "clf.fit(X_train, y_train_int)\n",
    "y_pred = clf.predict(X_val)\n",
    "accuracy = accuracy_score(y_val_int, y_pred)\n",
    "f1 = f1_score(y_val_int, y_pred, average='weighted')\n",
    "print(\"=============================================================================\")\n",
    "print(\"Random Forest with SpaceID Results:\")\n",
    "print(f\"\\nRandom Forest Accuracy with SpaceID: {accuracy:.4f}\")\n",
    "print(f\"Random Forest F1 Score with SpaceID: {f1:.4f}\\n\")\n",
    "\n",
    "#=============================================================================\n",
    "# Random Forest without SpaceID\n",
    "#=============================================================================\n",
    "#X_train_tot_split from earlier in the notebook\n",
    "#X_test_tot_split from earlier in the notebook\n",
    "\n",
    "X_preprocessor_clf = make_preprocessor(scale=False)\n",
    "\n",
    "X_std_train_wospaceID = X_preprocessor_clf.fit_transform(X_train_tot_split)\n",
    "X_std_test_wospaceID = X_preprocessor_clf.transform(X_test_tot_split)\n",
    "\n",
    "y_train_codes_wospaceID = Y_train_tot_split['LOC_CODE'].cat.codes\n",
    "y_test_codes_wospaceID = Y_test_tot_split['LOC_CODE'].cat.codes\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Training Random Forest without SpaceID...\")\n",
    "\n",
    "clf_wo_spaceID = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1, max_features='log2')\n",
    "clf_wo_spaceID.fit(X_std_train_wospaceID, y_train_codes_wospaceID)\n",
    "y_pred_wo_spaceID = clf_wo_spaceID.predict(X_std_test_wospaceID)\n",
    "accuracy_wo_spaceID = accuracy_score(y_test_codes_wospaceID, y_pred_wo_spaceID)\n",
    "f1_wo_spaceID = f1_score(y_test_codes_wospaceID, y_pred_wo_spaceID, average='weighted')\n",
    "print(\"=============================================================================\")\n",
    "print(\"Random Forest without SpaceID Results:\")\n",
    "print(f\"\\nRandom Forest without SpaceID Accuracy: {accuracy_wo_spaceID:.4f}\")\n",
    "print(f\"Random Forest without SpaceID F1 Score: {f1_wo_spaceID:.4f}\")\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Training Random Forest with initial Split...\")\n",
    "X_preprocessor_init_clf = make_preprocessor(scale=False)\n",
    "X_std_train_init = X_preprocessor_init_clf.fit_transform(X_raw_train)\n",
    "X_std_test_init = X_preprocessor_init_clf.transform(X_raw_test) \n",
    "y_train_init_clf = y_train_init['LOC_CODE'].astype('category')\n",
    "y_test_init_clf = y_test_init['LOC_CODE'].astype('category')\n",
    "y_train_init_codes = y_train_init_clf.cat.codes\n",
    "y_test_init_codes = y_test_init_clf.cat.codes\n",
    "\n",
    "clf_init = RandomForestClassifier(n_estimators=500, random_state=42, n_jobs=-1, max_features='log2')\n",
    "clf_init.fit(X_std_train_init, y_train_init_codes)\n",
    "y_pred_init = clf_init.predict(X_std_test_init)\n",
    "accuracy_init = accuracy_score(y_test_init_codes, y_pred_init)\n",
    "f1_init = f1_score(y_test_init_codes, y_pred_init, average='weighted')\n",
    "print(\"=============================================================================\")\n",
    "print(\"Random Forest with initial Split Results:\")\n",
    "print(f\"\\nRandom Forest with initial Split Accuracy: {accuracy_init:.4f}\")\n",
    "print(f\"Random Forest with initial Split F1 Score: {f1_init:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d01a88f",
   "metadata": {},
   "source": [
    "## XGBoost using parameter from Grid Search\n",
    "#### single run for preprocessing differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9211bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "\n",
    "# Use the train/validation split already computed earlier in the notebook\n",
    "# (X_train_split_sID, X_test_split_sID, Y_train_split_sID, Y_test_split_sID)\n",
    "\n",
    "X_train = X_train_split_sID\n",
    "X_val = X_test_split_sID\n",
    "y_train = Y_train_split_sID['LOC_CODE'].astype('category')\n",
    "y_val = Y_test_split_sID['LOC_CODE'].astype('category')\n",
    "\n",
    "# Integer-coded labels for models that require them (e.g., some XGBoost multiclass setups)\n",
    "y_train_int = y_train.cat.codes\n",
    "y_val_int = y_val.cat.codes\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# =================================================================================================================================================\n",
    "# XGBoost with SpaceID\n",
    "# =============================================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training XGBoost with SpaceID...\")\n",
    "xgb_clf = XGBClassifier(eval_metric='mlogloss', objective='multi:softprob', random_state=42, n_jobs=-1, learning_rate=0.1, n_estimators=500, max_depth=10)\n",
    "xgb_clf.fit(X_train, y_train_int)\n",
    "y_pred_xgb = xgb_clf.predict(X_val)\n",
    "accuracy_xgb = accuracy_score(y_val_int, y_pred_xgb)\n",
    "f1_xgb_wo_scaling = f1_score(y_val_int, y_pred_xgb, average='weighted')\n",
    "print(\"=============================================================================\")\n",
    "print(\"XGBoost with SpaceID Results:\")\n",
    "print(f\"\\nXGBoost Accuracy with SpaceID: {accuracy_xgb:.4f}\")\n",
    "print(f\"XGBoost F1 Score with SpaceID: {f1_xgb_wo_scaling:.4f}\\n\")\n",
    "\n",
    "#=============================================================================\n",
    "# XGBoost without SpaceID\n",
    "#=============================================================================\n",
    "#X_train_tot_split from earlier in the notebook\n",
    "#X_test_tot_split from earlier in the notebook\n",
    "\n",
    "X_preprocessor_woSpaceID_xgb_clf = make_preprocessor(scale=False)\n",
    "X_std_train_wospaceID = X_preprocessor_woSpaceID_xgb_clf.fit_transform(X_train_tot_split)\n",
    "X_std_test_wospaceID = X_preprocessor_woSpaceID_xgb_clf.transform(X_test_tot_split)\n",
    "\n",
    "y_train_codes_wospaceID = Y_train_tot_split['LOC_CODE'].cat.codes\n",
    "y_test_codes_wospaceID = Y_test_tot_split['LOC_CODE'].cat.codes\n",
    "\n",
    "\n",
    "\n",
    "print(\"Training XGBoost without SpaceID...\")\n",
    "\n",
    "xgb_clf_wo_spaceID = XGBClassifier(eval_metric='mlogloss', objective='multi:softprob', random_state=42 , learning_rate=0.1, n_estimators=500, max_depth=10, n_jobs=-1) \n",
    "xgb_clf_wo_spaceID.fit(X_std_train_wospaceID, y_train_codes_wospaceID)\n",
    "y_pred_xgb_wo_spaceID = xgb_clf_wo_spaceID.predict(X_std_test_wospaceID)\n",
    "accuracy_xgb_wo_spaceID = accuracy_score(y_test_codes_wospaceID, y_pred_xgb_wo_spaceID)\n",
    "f1_xgb_wo_spaceID = f1_score(y_test_codes_wospaceID, y_pred_xgb_wo_spaceID, average='weighted')\n",
    "print(\"=============================================================================\")\n",
    "print(\"XGBoost without SpaceID Results:\")\n",
    "print(f\"\\nXGBoost without SpaceID Accuracy: {accuracy_xgb_wo_spaceID:.4f}\")\n",
    "print(f\"XGBoost without SpaceID F1 Score: {f1_xgb_wo_spaceID:.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Training XGBoost with initial Split...\")\n",
    "X_preprocessor_init_xgb_clf = make_preprocessor(scale=False)\n",
    "X_std_train_init = X_preprocessor_init_xgb_clf.fit_transform(X_raw_train)\n",
    "X_std_test_init = X_preprocessor_init_xgb_clf.transform(X_raw_test) \n",
    "y_train_init_xgb_clf = y_train_init['LOC_CODE'].astype('category')\n",
    "y_test_init_xgb_clf = y_test_init['LOC_CODE'].astype('category')\n",
    "y_train_init_codes = y_train_init_xgb_clf.cat.codes\n",
    "y_test_init_codes = y_test_init_xgb_clf.cat.codes\n",
    "\n",
    "xgb_clf_init = XGBClassifier(eval_metric='mlogloss', objective='multi:softprob', random_state=42, learning_rate=0.1, n_estimators=500, max_depth=10, n_jobs=-1)\n",
    "xgb_clf_init.fit(X_std_train_init, y_train_init_codes)\n",
    "y_pred_init = xgb_clf_init.predict(X_std_test_init)\n",
    "accuracy_init = accuracy_score(y_test_init_codes, y_pred_init)\n",
    "f1_init = f1_score(y_test_init_codes, y_pred_init, average='weighted')\n",
    "print(\"=============================================================================\")\n",
    "print(\"XGBoost with initial Split Results:\")\n",
    "print(f\"\\nXGBoost with initial Split Accuracy: {accuracy_init:.4f}\")\n",
    "print(f\"XGBoost with initial Split F1 Score: {f1_init:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
